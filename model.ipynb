{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAD8CAYAAABEtrEzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86ff1c3cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FILEPATH = '/media/vincent/home/data/'\n",
    "BALANCED_FILE = FILEPATH+'driving_log_balanced.csv'\n",
    "ORIGINAL_FILE = FILEPATH+'driving_log.csv'\n",
    "MODELPATH = FILEPATH+'model.h5'\n",
    "\n",
    "df = pd.read_csv(ORIGINAL_FILE)\n",
    "df.columns =['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n",
    "balanced = pd.DataFrame() \t# Balanced dataset\n",
    "bins = 1000 \t\t\t\t# N of bins\n",
    "bin_n = 500\t\t\t# N of examples to include in each bin (at most)\n",
    "\n",
    "balanced_num =[]\n",
    "start = 0\n",
    "for end in np.linspace(0, 1, num=bins):  \n",
    "    df_range = df[(np.absolute(df.steering) >= start) & (np.absolute(df.steering) < end)]\n",
    "    range_n = min(bin_n, df_range.shape[0])\n",
    "    balanced_num.append(range_n)\n",
    "    if range_n != 0:\n",
    "        balanced = pd.concat([balanced, df_range.sample(range_n)])\n",
    "    start = end\n",
    "\n",
    "balanced.to_csv(BALANCED_FILE, index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(balanced_num)), balanced_num, width=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from scipy.misc import imresize\n",
    "\n",
    "def equal_Hist(img):\n",
    "    # Equalization Histogram\n",
    "    img_temp = img.copy()\n",
    "    img_temp[:,:,0] = cv2.equalizeHist(img[:, :, 0])\n",
    "    img_temp[:,:,1] = cv2.equalizeHist(img[:, :, 1])\n",
    "    img_temp[:,:,2] = cv2.equalizeHist(img[:, :, 2])\n",
    "    return img_temp\n",
    "\n",
    "def crop(img, margin=0):\n",
    "    return img[margin:(img.shape[0]-margin), margin:(img.shape[1]-margin)]\n",
    "\n",
    "def blur(img):\n",
    "    gb = cv2.GaussianBlur(img, (5,5), 20.0)\n",
    "    return cv2.addWeighted(img, 2, gb, -1, 0)\n",
    "\n",
    "def resize(image, new_size): \n",
    "    return imresize(image, new_size) \n",
    "\n",
    "def shadow(image):\n",
    "    img = np.copy(image)\n",
    "    h, w = image.shape[0], image.shape[1]\n",
    "    [x1, x2] = np.random.choice(w, 2, replace=False)\n",
    "    k = h / (x2 - x1)\n",
    "    b = - k * x1\n",
    "    for i in range(h):\n",
    "        c = int((i - b) / k)\n",
    "        img[i, :c, :] = (image[i, :c, :] * .5).astype(np.int32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def random_gamma(image):\n",
    "    \"\"\"\n",
    "    Random gamma correction is used as an alternative method changing the brightness of\n",
    "    training images.\n",
    "    http://www.pyimagesearch.com/2015/10/05/opencv-gamma-correction/\n",
    "    :param image:\n",
    "        Source image\n",
    "    :return:\n",
    "        New image generated by applying gamma correction to the source image\n",
    "    \"\"\"\n",
    "    gamma = np.random.uniform(0.4, 1.5)\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** inv_gamma) * 255\n",
    "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "\n",
    "\n",
    "def process(img):\n",
    "    result = random_gamma(img)\n",
    "    result = shadow(result)\n",
    "    result = resize(result, (80, 160, 3))\n",
    "    result = blur(result)\n",
    "    \n",
    "    #result = blur(img)\n",
    "    result = equal_Hist(result)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "samples = []\n",
    "with open(BALANCED_FILE) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples[1:], test_size=0.2)\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(lines, batch_size=32):\n",
    "    adjust_angle=[0, 0.2, -0.2]\n",
    "    while True:\n",
    "        shuffle(lines)\n",
    "        x_batch, y_batch = [], []\n",
    "        idx = 0\n",
    "        for line in lines:\n",
    "            #for i in range(3):\n",
    "            i = np.random.randint(0, 3)\n",
    "            name = FILEPATH+'IMG/'+line[i].split('/')[-1]\n",
    "            image = cv2.imread(name)\n",
    "            processed_image = process(image)\n",
    "            #print(np.array(processed_image).shape)\n",
    "\n",
    "            angle = float(line[3])\n",
    "\n",
    "            x_batch.append(processed_image)\n",
    "            y_batch.append(angle+adjust_angle[i])\n",
    "\n",
    "            x_batch.append(cv2.flip(processed_image,1))\n",
    "            y_batch.append((angle+adjust_angle[i])*-1.0)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "            if idx == batch_size:\n",
    "                yield np.array(x_batch), np.array(y_batch)\n",
    "                x_batch, y_batch = [], []\n",
    "                idx = 0\n",
    "\n",
    "train_generator = load_data(train_samples, batch_size=32)\n",
    "validation_generator = load_data(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom keras.models import Sequential\\nfrom keras.layers import Flatten, Dense, Lambda\\nfrom keras.layers import Conv2D, Dropout,\\nfrom keras.layers.pooling import MaxPooling2D\\nfrom keras.layers import Cropping2D\\n\\nmodel = Sequential()\\nmodel.add(Lambda(lambda x:x/255.0-0.5, input_shape=(80, 160, 3)))\\nmodel.add(Cropping2D(cropping=((35,12), (0,0))))\\n#model.add(Lambda(lambda x:x/255.0-0.5, input_shape=(160, 320, 3)))\\n#model.add(Cropping2D(cropping=((70,25), (0,0))))\\n\\nmodel.add(Conv2D(16,(3,3),activation='relu'))\\nmodel.add(MaxPooling2D(2,2))\\nmodel.add(Conv2D(32,(3,3),activation='relu'))\\nmodel.add(MaxPooling2D(2,2))\\nmodel.add(Conv2D(64,(3,3),activation='relu'))\\nmodel.add(MaxPooling2D(2,2))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(500))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(100))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(20))\\nmodel.add(Dense(1))\\n\\n#model.load_weights(MODELPATH)\\nmodel.compile(loss='mse', optimizer='adam')\\nmodel.fit_generator(train_generator, steps_per_epoch= len(train_samples), validation_data=validation_generator, nb_val_samples=len(validation_samples), nb_epoch=3)\\n\\nmodel.save(MODELPATH)\\n#exit()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.layers import Conv2D, Dropout,\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Cropping2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x:x/255.0-0.5, input_shape=(80, 160, 3)))\n",
    "model.add(Cropping2D(cropping=((35,12), (0,0))))\n",
    "#model.add(Lambda(lambda x:x/255.0-0.5, input_shape=(160, 320, 3)))\n",
    "#model.add(Cropping2D(cropping=((70,25), (0,0))))\n",
    "\n",
    "model.add(Conv2D(16,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#model.load_weights(MODELPATH)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(train_generator, steps_per_epoch= len(train_samples), validation_data=validation_generator, nb_val_samples=len(validation_samples), nb_epoch=3)\n",
    "\n",
    "model.save(MODELPATH)\n",
    "#exit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), strides=(2, 2), padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), strides=(2, 2), padding=\"same\")`\n",
      "  app.launch_new_instance()\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), strides=(2, 2), padding=\"same\")`\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\")`\n",
      "/home/vincent/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=1459, validation_data=<generator..., callbacks=[<keras.ca..., epochs=20, validation_steps=3650)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1459/1459 [==============================] - 435s - loss: 0.0667 - val_loss: 0.0597\n",
      "Epoch 2/20\n",
      "1459/1459 [==============================] - 418s - loss: 0.0559 - val_loss: 0.0575\n",
      "Epoch 3/20\n",
      "1459/1459 [==============================] - 416s - loss: 0.0545 - val_loss: 0.0539\n",
      "Epoch 4/20\n",
      "1459/1459 [==============================] - 416s - loss: 0.0524 - val_loss: 0.0529\n",
      "Epoch 5/20\n",
      "1459/1459 [==============================] - 419s - loss: 0.0518 - val_loss: 0.0532\n",
      "Epoch 6/20\n",
      "1459/1459 [==============================] - 419s - loss: 0.0503 - val_loss: 0.0516\n",
      "Epoch 7/20\n",
      "1459/1459 [==============================] - 421s - loss: 0.0492 - val_loss: 0.0511\n",
      "Epoch 8/20\n",
      "1459/1459 [==============================] - 428s - loss: 0.0484 - val_loss: 0.0503\n",
      "Epoch 9/20\n",
      "1459/1459 [==============================] - 428s - loss: 0.0479 - val_loss: 0.0526\n",
      "Epoch 10/20\n",
      "1459/1459 [==============================] - 427s - loss: 0.0474 - val_loss: 0.0507\n",
      "Epoch 11/20\n",
      "1459/1459 [==============================] - 415s - loss: 0.0472 - val_loss: 0.0490\n",
      "Epoch 12/20\n",
      "1459/1459 [==============================] - 421s - loss: 0.0465 - val_loss: 0.0505\n",
      "Epoch 13/20\n",
      "1459/1459 [==============================] - 425s - loss: 0.0462 - val_loss: 0.0492\n",
      "Epoch 14/20\n",
      "1459/1459 [==============================] - 421s - loss: 0.0464 - val_loss: 0.0486\n",
      "Epoch 15/20\n",
      "1459/1459 [==============================] - 426s - loss: 0.0454 - val_loss: 0.0486\n",
      "Epoch 16/20\n",
      "1459/1459 [==============================] - 417s - loss: 0.0456 - val_loss: 0.0478\n",
      "Epoch 17/20\n",
      "1459/1459 [==============================] - 417s - loss: 0.0447 - val_loss: 0.0479\n",
      "Epoch 18/20\n",
      "1459/1459 [==============================] - 417s - loss: 0.0452 - val_loss: 0.0490\n",
      "Epoch 19/20\n",
      "1459/1459 [==============================] - 417s - loss: 0.0441 - val_loss: 0.0472\n",
      "Epoch 20/20\n",
      "1459/1459 [==============================] - 417s - loss: 0.0438 - val_loss: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86a64b5588>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.layers import Conv2D, Dropout,Convolution2D,Activation\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Cropping2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x:x/255.0-0.5, input_shape=(80, 160, 3)))\n",
    "model.add(Cropping2D(cropping=((35,12), (0,0))))\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Convolution2D(36, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation(activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Convolution2D(48, 5, 5, border_mode='same', subsample=(2, 2)))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', subsample=(1, 1)))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Next, five fully connected layers\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1))\n",
    "    \n",
    "checkpointer = ModelCheckpoint(FILEPATH+'weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "\n",
    "#model.load_weights(MODELPATH)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(train_generator, steps_per_epoch= int(len(train_samples)/10), \n",
    "                    validation_data=validation_generator, nb_val_samples=len(validation_samples),\n",
    "                    nb_epoch=20, callbacks=[checkpointer])\n",
    "\n",
    "#model.save(MODELPATH)\n",
    "#exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
